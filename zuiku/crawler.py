import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict, Optional, Tuple  # âœ… åˆå¹¶æ‰€æœ‰ç±»å‹å¯¼å…¥
from config import CRAWLER_CONFIG, USER_AGENT
import re
from urllib.parse import urljoin
from datetime import datetime, date
from utils.event_level_extractor import EventLevelExtractorV2  # âœ¨ æ–°å¢ï¼šä»HTMLæ ‡ç­¾æå–


logger = logging.getLogger(__name__)


class RaceCrawler:
    """èµ›äº‹çˆ¬è™«"""

    def __init__(self):
        self.base_url = CRAWLER_CONFIG['base_url']
        self.events_url = CRAWLER_CONFIG['events_url']
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        self._consecutive_old_pages = 0  # æå‰ç»ˆæ­¢è®¡æ•°å™¨

    def get_page(self, url: str, retry: int = 0) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(
                url,
                timeout=CRAWLER_CONFIG['timeout']
            )
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            if retry < CRAWLER_CONFIG['retry_times']:
                logger.warning(f"è·å–é¡µé¢å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{CRAWLER_CONFIG['retry_times']}: {url}")
                time.sleep(2 ** retry)
                return self.get_page(url, retry + 1)
            else:
                logger.error(f"è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
                return None

    def get_event_list_urls(self, max_pages: int = None) -> List[str]:
        """è·å–æ‰€æœ‰èµ›äº‹åˆ—è¡¨é¡µçš„URL"""
        max_pages = max_pages or CRAWLER_CONFIG['max_pages']
        urls = []

        for page in range(1, max_pages + 1):
            url = f"{self.events_url}?page={page}&per-page={CRAWLER_CONFIG['per_page']}"
            urls.append(url)

        return urls

    def parse_categories(self, html: str) -> Tuple[str, List[Dict]]:
        """
        è§£æç»„åˆ«åˆ—è¡¨ï¼Œæå–èµ›äº‹æ•´ä½“çŠ¶æ€å’Œæ‰€æœ‰ç»„åˆ«ä¿¡æ¯

        ä»è¯¦æƒ…é¡µç»„åˆ«HTMLæå–ï¼š
        1. èµ›äº‹æ•´ä½“çŠ¶æ€ï¼ˆæŠ¥åä¸­/å·²æˆªæ­¢/å·²ç»“æŸï¼‰
        2. æ¯ä¸ªç»„åˆ«çš„æŠ¥åçŠ¶æ€å’Œé“¾æ¥

        Args:
            html: è¯¦æƒ…é¡µHTML

        Returns:
            tuple: (event_status, categories_list)
            - event_status: èµ›äº‹æ•´ä½“çŠ¶æ€
            - categories_list: ç»„åˆ«ä¿¡æ¯åˆ—è¡¨
        """
        from utils.event_status_extractor import EventStatusExtractor

        # ä½¿ç”¨æ–°çš„çŠ¶æ€æå–å™¨
        event_status, categories_info = EventStatusExtractor.parse_all_categories_status(html)

        logger.info(f"âœ“ èµ›äº‹æ•´ä½“çŠ¶æ€: {event_status}")
        logger.info(f"âœ“ æ‰¾åˆ°ç»„åˆ«æ•°é‡: {len(categories_info)}")

        # è§£ææ¯ä¸ªç»„åˆ«çš„è¯¦ç»†ä¿¡æ¯
        categories = []

        for idx, cat_info in enumerate(categories_info, 1):
            try:
                cat_soup = BeautifulSoup(cat_info['html'], 'html.parser')

                # æå–ç»„åˆ«åç§°
                name_tag = cat_soup.find('h4', class_='name')
                name = name_tag.get_text(strip=True) if name_tag else None

                # ä»åç§°ä¸­æå–è·ç¦»ï¼ˆå¦‚æœæœ‰æ‹¬å·ï¼‰
                distance = None
                if name and 'ï¼ˆ' in name and 'ï¼‰' in name:
                    match = re.search(r'ï¼ˆ(.+?)ï¼‰', name)
                    if match:
                        distance = match.group(1)

                # æå–ä»·æ ¼
                price_div = cat_soup.find('div', class_='price')
                fee = price_div.get_text(strip=True) if price_div else None

                # æå–å…¶ä»–å¯èƒ½çš„ä¿¡æ¯
                highlight = cat_soup.find('small', class_='highlight_short')
                highlight_text = highlight.get_text(strip=True) if highlight else None

                # ç»„åˆæ•°æ®
                category = {
                    'name': name,
                    'distance': distance,
                    'fee': fee,
                    'highlight': highlight_text,
                    'registration_status': cat_info['registration_status'],
                    'registration_url': cat_info['registration_url']
                }

                categories.append(category)

                logger.info(f"  ç»„åˆ«{idx}: {name} - "
                            f"çŠ¶æ€:{cat_info['registration_status']} - "
                            f"é“¾æ¥:{cat_info['registration_url'][:50] if cat_info['registration_url'] else 'None'}...")

            except Exception as e:
                logger.error(f"è§£æç»„åˆ«{idx}å¤±è´¥: {e}")
                continue

        return event_status, categories

    def parse_event_list(self, html: str) -> List[Dict]:
        """è§£æèµ›äº‹åˆ—è¡¨é¡µï¼Œæå–èµ›äº‹URL"""
        soup = BeautifulSoup(html, 'html.parser')
        events = []

        # æ‰¾åˆ°æ‰€æœ‰èµ›äº‹é¡¹
        event_divs = soup.find_all('div', class_='event')

        for event_div in event_divs:
            try:
                # è·å–èµ›äº‹é“¾æ¥
                name_tag = event_div.find('h4', class_='name')
                if not name_tag:
                    continue

                link_tag = name_tag.find('a', class_='event-a')
                if not link_tag:
                    continue

                event_url = link_tag.get('href')
                event_name = link_tag.get_text(strip=True)

                # æå–event_id
                event_id = event_url.split('/')[-1] if event_url else None

                if not event_id or not event_url:
                    continue

                # è·å–åŸºæœ¬ä¿¡æ¯
                info_div = event_div.find('div', class_='info')
                info_text = info_div.get_text(strip=True) if info_div else ''

                # æå–èµ›äº‹æ—¥æœŸ
                event_date = None
                if info_div:
                    info_p = info_div.find('p')
                    if info_p:
                        info_p_text = info_p.get_text()
                        # åŒ¹é…æ—¥æœŸï¼š2026.03.15 æˆ– 2026-03-15
                        date_match = re.search(r'(\d{4})[.\-](\d{2})[.\-](\d{2})', info_p_text)
                        if date_match:
                            year = date_match.group(1)
                            month = date_match.group(2)
                            day = date_match.group(3)
                            event_date = f"{year}-{month}-{day}"

                # è·å–æŠ¥åæˆªæ­¢æ—¶é—´
                meta_div = event_div.find('div', class_='meta')
                deadline = ''
                if meta_div:
                    deadline_span = meta_div.find('span', string=lambda t: t and 'æŠ¥åæˆªæ­¢' in t)
                    if deadline_span:
                        deadline = deadline_span.get_text(strip=True)

                # âœ¨ æå–èµ›äº‹çº§åˆ«ï¼ˆä»HTMLæ ‡ç­¾ï¼‰
                event_level = EventLevelExtractorV2.extract_event_level_from_html(str(event_div))

                events.append({
                    'event_id': event_id,
                    'event_url': urljoin(self.base_url, event_url),
                    'name': event_name,
                    'basic_info': info_text,
                    'registration_deadline': deadline,
                    'event_date': event_date,
                    'event_level': event_level  # âœ¨ æ–°å¢ï¼šèµ›äº‹çº§åˆ«
                })

                logger.debug(f"è§£æåˆ°èµ›äº‹: {event_name}, æ—¥æœŸ: {event_date}, URL: {event_url}, ID: {event_id}")

            except Exception as e:
                logger.error(f"è§£æèµ›äº‹é¡¹å¤±è´¥: {e}")
                continue

        return events

    def parse_event_detail(self, html: str, event_url: str, event_id: str) -> Dict:
        """è§£æèµ›äº‹è¯¦æƒ…é¡µ

        Args:
            html: é¡µé¢HTML
            event_url: èµ›äº‹è¯¦æƒ…é¡µURL
            event_id: èµ›äº‹ID
        """
        soup = BeautifulSoup(html, 'html.parser')
        detail = {
            'raw_html': html,
            'event_url': event_url
        }

        try:
            # è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆä½œä¸ºè¯¦æƒ…æ–‡æœ¬ï¼‰
            # ç§»é™¤scriptã€styleç­‰æ ‡ç­¾
            for tag in soup(['script', 'style', 'header', 'footer', 'nav']):
                tag.decompose()

            # è·å–ä¸»è¦å†…å®¹åŒºåŸŸçš„æ–‡æœ¬
            main_content = soup.find('div', class_='container') or soup.find('main') or soup.body
            if main_content:
                detail['detail_text'] = main_content.get_text(strip=True, separator='\n')
                logger.debug(f"è·å–åˆ°è¯¦æƒ…é¡µæ–‡æœ¬ï¼Œé•¿åº¦: {len(detail['detail_text'])} å­—ç¬¦")

            # è·å–èµ›äº‹åç§°
            title_tag = soup.find('h1') or soup.find('h2', class_='event-title')
            if title_tag:
                detail['name'] = title_tag.get_text(strip=True)

            # è·å–èµ›äº‹ä»‹ç»
            desc_div = soup.find('div', class_='event-description') or \
                       soup.find('div', class_='description') or \
                       soup.find('div', class_='event-detail')
            if desc_div:
                detail['description'] = desc_div.get_text(strip=True)

            # è·å–èµ›äº‹æ—¶é—´å’Œåœ°ç‚¹
            info_section = soup.find('div', class_='event-info')
            if info_section:
                info_items = info_section.find_all('p')
                for item in info_items:
                    text = item.get_text(strip=True)
                    if 'æ—¶é—´' in text or 'æ—¥æœŸ' in text:
                        detail['event_date'] = text
                    elif 'åœ°ç‚¹' in text or 'åœ°å€' in text:
                        detail['location'] = text

            # è·å–ç»„åˆ«ä¿¡æ¯
            categories_section = soup.find('div', class_='race-categories') or \
                                 soup.find('section', string=lambda t: t and 'ç»„åˆ«' in t)
            if categories_section:
                categories = []
                category_items = categories_section.find_all('li') or \
                                 categories_section.find_all('div', class_='category')
                for item in category_items:
                    categories.append(item.get_text(strip=True))
                detail['race_categories_text'] = categories

            # âœ¨âœ¨âœ¨ æ–°å¢ï¼šè§£æç»„åˆ«åˆ—è¡¨å’Œèµ›äº‹æ•´ä½“çŠ¶æ€ âœ¨âœ¨âœ¨
            event_status, categories = self.parse_categories(html)
            detail['status'] = event_status  # èµ›äº‹æ•´ä½“çŠ¶æ€
            detail['categories'] = categories  # ç»„åˆ«åˆ—è¡¨

            logger.info(f"âœ“ èµ›äº‹çŠ¶æ€: {event_status}, ç»„åˆ«æ•°é‡: {len(categories)}")
            # âœ¨âœ¨âœ¨ æ–°å¢ç»“æŸ âœ¨âœ¨âœ¨

            # é‡è¦ï¼šæŸ¥æ‰¾èµ„è®¯é“¾æ¥
            # æ–¹æ³•1: æŸ¥æ‰¾hrefåŒ…å« /news/archives/tag/event çš„é“¾æ¥
            news_link = soup.find('a', href=re.compile(r'/news/archives/tag/event\d+'))
            if news_link:
                href = news_link.get('href')
                detail['news_url'] = urljoin(self.base_url, href)
                logger.info(f"âœ“ æ‰¾åˆ°èµ„è®¯é“¾æ¥ï¼ˆæ–¹æ³•1ï¼‰: {detail['news_url']}")
            else:
                # æ–¹æ³•2: æ ¹æ®event_idæ„é€ èµ„è®¯URL
                constructed_url = f"https://zuicool.com/news/archives/tag/event{event_id}"
                detail['news_url'] = constructed_url
                logger.info(f"âœ“ æ„é€ èµ„è®¯é“¾æ¥ï¼ˆæ–¹æ³•2ï¼‰: {detail['news_url']}")

        except Exception as e:
            logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥: {e}")

        return detail

    def get_news_list_content(self, news_list_url: str) -> Dict:
        """è·å–èµ„è®¯åˆ—è¡¨é¡µé¢çš„æ‰€æœ‰æ–‡ç« å†…å®¹

        Args:
            news_list_url: èµ„è®¯åˆ—è¡¨é¡µURLï¼Œå¦‚ https://zuicool.com/news/archives/tag/event81730

        Returns:
            åŒ…å«æ‰€æœ‰æ–‡ç« å†…å®¹å’Œå›¾ç‰‡çš„å­—å…¸
        """
        html = self.get_page(news_list_url)
        if not html:
            logger.warning(f"æ— æ³•è·å–èµ„è®¯åˆ—è¡¨é¡µ: {news_list_url}")
            return {'text': '', 'images': [], 'articles': []}

        soup = BeautifulSoup(html, 'html.parser')
        all_content = {
            'news_list_url': news_list_url,
            'text': '',
            'images': [],
            'articles': []
        }

        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« é“¾æ¥
            article_links = []

            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰æŒ‡å‘ /news/archives/æ•°å­— çš„é“¾æ¥
            all_links = soup.find_all('a', href=re.compile(r'/news/archives/\d+$'))
            for link in all_links:
                article_url = urljoin(self.base_url, link.get('href'))
                if article_url not in article_links:
                    article_links.append(article_url)

            # æ–¹æ³•2: æŸ¥æ‰¾æ–‡ç« æ ‡é¢˜é“¾æ¥
            if not article_links:
                articles = soup.find_all('article')
                for article in articles:
                    title_link = article.find('a', class_='entry-title-link') or \
                                 article.find('h2').find('a') if article.find('h2') else None
                    if title_link:
                        article_url = urljoin(self.base_url, title_link.get('href'))
                        if article_url not in article_links:
                            article_links.append(article_url)

            logger.info(f"åœ¨èµ„è®¯åˆ—è¡¨é¡µæ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« ")

            if not article_links:
                logger.warning(f"æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥ï¼Œå¯èƒ½é¡µé¢ç»“æ„ä¸åŒ: {news_list_url}")
                return all_content

            # è·å–æ¯ç¯‡æ–‡ç« çš„å†…å®¹ï¼ˆæœ€å¤š5ç¯‡ï¼‰
            for idx, article_url in enumerate(article_links[:5], 1):
                logger.info(f"è·å–æ–‡ç«  {idx}/{min(len(article_links), 5)}: {article_url}")
                time.sleep(CRAWLER_CONFIG['delay'])

                article_content = self.get_article_content(article_url)
                if article_content and article_content['text']:
                    all_content['articles'].append(article_content)
                    all_content['text'] += f"\n\n=== æ–‡ç«  {idx}: {article_url} ===\n{article_content['text']}\n"
                    all_content['images'].extend(article_content['images'])
                else:
                    logger.warning(f"æ–‡ç« å†…å®¹ä¸ºç©º: {article_url}")

            logger.info(f"å…±è·å–åˆ°æ–‡æœ¬é•¿åº¦: {len(all_content['text'])} å­—ç¬¦")
            logger.info(f"å…±è·å–åˆ° {len(all_content['images'])} å¼ å›¾ç‰‡")

        except Exception as e:
            logger.error(f"è§£æèµ„è®¯åˆ—è¡¨é¡µå¤±è´¥: {e}")

        return all_content

    def get_article_content(self, article_url: str) -> Dict:
        """è·å–å•ç¯‡æ–‡ç« çš„å†…å®¹ - åªä»entry-contentè·å–"""
        html = self.get_page(article_url)
        if not html:
            return {'article_url': article_url, 'text': '', 'images': []}

        soup = BeautifulSoup(html, 'html.parser')
        content = {
            'article_url': article_url,
            'text': '',
            'images': []
        }

        try:
            # åªè·å–entry-contentä¸­çš„å†…å®¹
            entry_content = soup.find('div', class_='entry-content')

            if entry_content:
                # è·å–æ–‡æœ¬å†…å®¹
                # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                for script in entry_content(['script', 'style']):
                    script.decompose()

                content['text'] = entry_content.get_text(strip=True, separator='\n')

                # åªè·å–entry-contentä¸­çš„å›¾ç‰‡
                img_tags = entry_content.find_all('img')
                for img in img_tags:
                    src = img.get('src') or img.get('data-src')
                    if src:
                        # è¿‡æ»¤æ‰å°å›¾æ ‡å’Œlogo
                        src_lower = src.lower()
                        if any(keyword in src_lower for keyword in ['logo', 'icon', 'avatar', 'favicon']):
                            continue

                        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸å±æ€§
                        width = img.get('width')
                        height = img.get('height')

                        if width and height:
                            try:
                                if int(width) < 100 or int(height) < 100:
                                    continue
                            except (ValueError, TypeError):
                                pass

                        full_url = urljoin(self.base_url, src)
                        content['images'].append(full_url)

                logger.debug(f"ä»æ–‡ç« ä¸­æå–: æ–‡æœ¬ {len(content['text'])} å­—ç¬¦, å›¾ç‰‡ {len(content['images'])} å¼ ")
            else:
                logger.warning(f"æœªæ‰¾åˆ°entry-content: {article_url}")

        except Exception as e:
            logger.error(f"è§£ææ–‡ç« é¡µå¤±è´¥: {e}")

        return content

    def crawl_all_events(self, min_date: str = None) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰èµ›äº‹ï¼ˆæ”¯æŒæ—¥æœŸè¿‡æ»¤ + æ™ºèƒ½åœæ­¢ï¼‰

        å› ä¸ºç½‘ç«™æ’åºæ··ä¹±ï¼Œæé«˜åœæ­¢é˜ˆå€¼åˆ°20é¡µ

        Args:
            min_date: æœ€å°æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'ï¼Œä¸ä¼ åˆ™çˆ¬å–æ‰€æœ‰èµ›äº‹
        """
        # åªæœ‰ä¼ äº†min_dateæ‰è¿‡æ»¤æ—¥æœŸ
        min_date_obj = None
        if min_date:
            min_date_obj = datetime.strptime(min_date, '%Y-%m-%d').date()
            logger.info(f"ğŸ“… æ—¶é—´è¿‡æ»¤: >= {min_date}")
        else:
            logger.info(f"âš™ï¸  ä¸è¿‡æ»¤æ—¥æœŸï¼Œçˆ¬å–åˆ°æœ€åä¸€é¡µæˆ–200é¡µä¸ºæ­¢")

        logger.info(f"âš ï¸  ç½‘ç«™æ’åºæ··ä¹±ï¼Œæé«˜åœæ­¢é˜ˆå€¼åˆ°20é¡µ")

        all_events = []
        list_urls = self.get_event_list_urls()

        # æé«˜åœæ­¢é˜ˆå€¼ï¼ˆç½‘ç«™æ’åºæ··ä¹±ï¼‰
        STOP_THRESHOLD = 20

        logger.info(f"å¼€å§‹çˆ¬å– {len(list_urls)} ä¸ªåˆ—è¡¨é¡µ")
        if min_date_obj:
            logger.info(f"âš™ï¸  æå‰åœæ­¢é˜ˆå€¼: è¿ç»­ {STOP_THRESHOLD} é¡µæ— æœ‰æ•ˆèµ›äº‹")
        else:
            logger.info(f"âš™ï¸  ä¸è¿‡æ»¤æ—¥æœŸï¼Œçˆ¬å–åˆ°æœ€åä¸€é¡µæˆ–200é¡µä¸ºæ­¢")

        for idx, url in enumerate(list_urls, 1):
            logger.info(f"çˆ¬å–åˆ—è¡¨é¡µ {idx}/{len(list_urls)}: {url}")

            html = self.get_page(url)
            if not html:
                continue

            events = self.parse_event_list(html)
            logger.info(f"ä»åˆ—è¡¨é¡µè§£æåˆ° {len(events)} ä¸ªèµ›äº‹")

            # æ—¥æœŸè¿‡æ»¤ï¼ˆå¦‚æœæœ‰min_date_objï¼‰
            page_valid_count = 0
            for event in events:
                # å¦‚æœæ²¡æœ‰è®¾ç½®æ—¥æœŸè¿‡æ»¤ï¼Œæ‰€æœ‰èµ›äº‹éƒ½æœ‰æ•ˆ
                if min_date_obj is None:
                    all_events.append(event)
                    page_valid_count += 1
                elif self._is_event_valid_by_date(event, min_date_obj):
                    all_events.append(event)
                    page_valid_count += 1
                else:
                    logger.debug(f"è¿‡æ»¤å·²è¿‡æœŸèµ›äº‹: {event['name']} ({event.get('event_date', 'N/A')})")

            logger.info(f"è¿‡æ»¤åæœ¬é¡µæœ‰æ•ˆèµ›äº‹: {page_valid_count} ä¸ª")

            # æ™ºèƒ½åœæ­¢é€»è¾‘
            if min_date_obj is None:
                # ä¸è¿‡æ»¤æ—¥æœŸçš„æƒ…å†µï¼šåªè¦è¿™é¡µæ²¡æœ‰èµ›äº‹å°±åœæ­¢
                if not events:
                    logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
            else:
                # è¿‡æ»¤æ—¥æœŸçš„æƒ…å†µï¼šè¿ç»­20é¡µæ— æœ‰æ•ˆèµ›äº‹æ‰åœæ­¢
                if page_valid_count == 0:
                    self._consecutive_old_pages += 1
                    if self._consecutive_old_pages % 5 == 0:  # æ¯5é¡µæç¤ºä¸€æ¬¡
                        logger.info(f"è¿ç»­ {self._consecutive_old_pages} é¡µæ— æœ‰æ•ˆèµ›äº‹")

                    if self._consecutive_old_pages >= STOP_THRESHOLD:
                        logger.info(f"è¿ç»­{STOP_THRESHOLD}é¡µæ— æœ‰æ•ˆèµ›äº‹ï¼Œåœæ­¢çˆ¬å–")
                        break
                else:
                    self._consecutive_old_pages = 0

            # å»¶è¿Ÿ
            time.sleep(CRAWLER_CONFIG['delay'])

        logger.info(f"âœ“ å…±çˆ¬å–åˆ° {len(all_events)} ä¸ªæœ‰æ•ˆèµ›äº‹")
        return all_events

    def crawl_event_detail(self, event: Dict) -> Dict:
        """çˆ¬å–å•ä¸ªèµ›äº‹çš„è¯¦ç»†ä¿¡æ¯

        æµç¨‹ï¼š
        1. è·å–èµ›äº‹è¯¦æƒ…é¡µ
        2. ä»è¯¦æƒ…é¡µè·å–èµ„è®¯é“¾æ¥
        3. è·å–èµ„è®¯å†…å®¹å’Œå›¾ç‰‡
        """
        logger.info(f"â†’ çˆ¬å–èµ›äº‹è¯¦æƒ…: {event['name']}")
        logger.info(f"  è¯¦æƒ…URL: {event['event_url']}")

        # æ­¥éª¤1: è·å–è¯¦æƒ…é¡µ
        html = self.get_page(event['event_url'])
        if not html:
            logger.error(f"âœ— æ— æ³•è·å–è¯¦æƒ…é¡µ: {event['event_url']}")
            return event

        detail = self.parse_event_detail(html, event['event_url'], event['event_id'])
        event.update(detail)

        # æ­¥éª¤2: è·å–èµ„è®¯å†…å®¹
        if 'news_url' in event and event['news_url']:
            logger.info(f"â†’ è·å–èµ„è®¯å†…å®¹")
            logger.info(f"  èµ„è®¯URL: {event['news_url']}")

            time.sleep(CRAWLER_CONFIG['delay'])
            news_content = self.get_news_list_content(event['news_url'])

            event['news_content_raw'] = news_content.get('text', '')
            event['images'] = news_content.get('images', [])

            logger.info(f"âœ“ èµ„è®¯æ–‡æœ¬: {len(event['news_content_raw'])} å­—ç¬¦")
            logger.info(f"âœ“ èµ„è®¯å›¾ç‰‡: {len(event['images'])} å¼ ")
        else:
            logger.warning(f"âœ— æ²¡æœ‰æ‰¾åˆ°èµ„è®¯URL: {event['name']}")
            event['news_content_raw'] = ''
            event['images'] = []

        return event

    def _is_event_valid_by_date(self, event: dict, min_date_obj: date) -> bool:
        """åˆ¤æ–­èµ›äº‹æ˜¯å¦æœ‰æ•ˆï¼ˆæ—¥æœŸè¿‡æ»¤ï¼‰

        Args:
            event: èµ›äº‹ä¿¡æ¯
            min_date_obj: æœ€å°æ—¥æœŸå¯¹è±¡

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        event_date = event.get('event_date')
        if not event_date:
            # æ²¡æœ‰æ—¥æœŸçš„èµ›äº‹ä¿ç•™
            return True

        try:
            event_date_obj = datetime.strptime(event_date, '%Y-%m-%d').date()
            return event_date_obj >= min_date_obj
        except Exception as e:
            logger.warning(f"è§£ææ—¥æœŸå¤±è´¥: {event_date} - {e}")
            return True  # è§£æå¤±è´¥çš„ä¹Ÿä¿ç•™