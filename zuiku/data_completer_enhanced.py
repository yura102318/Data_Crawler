import logging
from typing import Dict, List, Optional
from config import QWEN_API_KEY, QWEN_API_BASE
import httpx
import json
import requests
import time
from bs4 import BeautifulSoup
from urllib.parse import quote

logger = logging.getLogger(__name__)


class EnhancedDataCompleter:
    """å¢å¼ºç‰ˆæ•°æ®è¡¥å…¨å™¨ - ä½¿ç”¨å¤šå±‚ç­–ç•¥è¡¥å…¨ç¼ºå¤±çš„èµ›äº‹ä¿¡æ¯"""

    def __init__(self):
        self.api_key = QWEN_API_KEY
        self.api_base = QWEN_API_BASE
        self.search_model = "qwen-plus"
        self.max_retries = 3  # æ¯å±‚ç­–ç•¥çš„æœ€å¤§é‡è¯•æ¬¡æ•°

    def check_missing_fields(self, structured_data: Dict) -> Dict:
        """æ£€æŸ¥å“ªäº›å…³é”®å­—æ®µç¼ºå¤±æˆ–ä¸å®Œæ•´"""
        missing = {
            'has_missing': False,
            'missing_fields': [],
            'incomplete_categories': [],
            'missing_total_scale': False
        }

        # æ£€æŸ¥èµ›äº‹æ€»è§„æ¨¡
        total_scale = structured_data.get('total_scale')
        if not total_scale or total_scale == 'null' or str(total_scale).strip() == '':
            missing['has_missing'] = True
            missing['missing_total_scale'] = True
            missing['missing_fields'].append('total_scale')
            logger.warning("âš  ç¼ºå¤±ï¼šèµ›äº‹æ€»è§„æ¨¡")

        # æ£€æŸ¥ç»„åˆ«ä¿¡æ¯
        race_categories = structured_data.get('race_categories', [])

        if not race_categories or len(race_categories) == 0:
            missing['has_missing'] = True
            missing['missing_fields'].append('race_categories')
            logger.warning("âš  ç¼ºå¤±ï¼šç»„åˆ«ä¿¡æ¯å®Œå…¨ä¸ºç©º")
        else:
            # æ£€æŸ¥æ¯ä¸ªç»„åˆ«çš„å®Œæ•´æ€§
            for idx, category in enumerate(race_categories):
                incomplete_fields = []

                if not category.get('name') or category.get('name') == 'null':
                    incomplete_fields.append('name')

                if not category.get('distance') or category.get('distance') == 'null':
                    incomplete_fields.append('distance')

                if not category.get('fee') or category.get('fee') == 'null':
                    incomplete_fields.append('fee')

                if not category.get('total_quota') or category.get('total_quota') == 'null':
                    incomplete_fields.append('total_quota')

                if incomplete_fields:
                    missing['has_missing'] = True
                    missing['incomplete_categories'].append({
                        'index': idx,
                        'category': category,
                        'missing_fields': incomplete_fields
                    })
                    logger.warning(f"âš  ç»„åˆ« {idx + 1} ({category.get('name', 'æœªçŸ¥')}) ç¼ºå¤±å­—æ®µ: {', '.join(incomplete_fields)}")

        return missing

    # ==================== ç¬¬ä¸€å±‚ï¼šä¼˜åŒ–çš„AIè”ç½‘æœç´¢ ====================

    def layer1_ai_search(self, event_name: str, missing_info: Dict, retry: int = 0) -> Optional[Dict]:
        """ç¬¬ä¸€å±‚ï¼šä½¿ç”¨AIè”ç½‘æœç´¢è¡¥å…¨æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        try:
            logger.info(f"ğŸ” [ç¬¬1å±‚] AIè”ç½‘æœç´¢ (å°è¯• {retry + 1}/{self.max_retries})")

            from openai import OpenAI

            client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                http_client=httpx.Client(
                    base_url=self.api_base,
                    follow_redirects=True,
                )
            )

            # æ„å»ºæ›´è¯¦ç»†çš„æœç´¢æç¤ºè¯
            prompt = self._build_enhanced_search_prompt(event_name, missing_info, retry)

            response = client.chat.completions.create(
                model=self.search_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é©¬æ‹‰æ¾èµ›äº‹ä¿¡æ¯æœé›†åŠ©æ‰‹ã€‚ä½ å¿…é¡»é€šè¿‡æœç´¢å¼•æ“æŸ¥æ‰¾èµ›äº‹ä¿¡æ¯ï¼Œå¹¶æå–å‡†ç¡®çš„ç»„åˆ«ã€è·ç¦»ã€æŠ¥åè´¹ç”¨å’Œåé¢ä¿¡æ¯ã€‚è¯·ç¡®ä¿ä¿¡æ¯å‡†ç¡®å¯é ï¼Œä¼˜å…ˆä»å®˜æ–¹å…¬ä¼—å·ã€å®˜ç½‘ç­‰æƒå¨æ¥æºè·å–ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result = response.choices[0].message.content
            completed_data = json.loads(result)

            # éªŒè¯è¿”å›çš„æ•°æ®è´¨é‡
            if self._validate_completed_data(completed_data, missing_info):
                logger.info(f"âœ“ [ç¬¬1å±‚] AIæœç´¢æˆåŠŸ")
                return completed_data
            else:
                logger.warning(f"âš  [ç¬¬1å±‚] AIè¿”å›æ•°æ®è´¨é‡ä¸è¶³")
                return None

        except Exception as e:
            logger.error(f"âœ— [ç¬¬1å±‚] AIæœç´¢å¤±è´¥: {e}")
            return None

    def _build_enhanced_search_prompt(self, event_name: str, missing_info: Dict, retry: int) -> str:
        """æ„å»ºå¢å¼ºç‰ˆæœç´¢æç¤ºè¯"""

        # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´æœç´¢ç­–ç•¥
        search_tips = [
            "è¯·æœç´¢å®˜æ–¹å…¬ä¼—å·æ–‡ç« ã€å®˜ç½‘ä¿¡æ¯",
            "è¯·å°è¯•æœç´¢èµ›äº‹æŠ¥åå¹³å°ï¼ˆå¦‚ï¼šæœ€é…·ã€é©¬æ‹‰é©¬æ‹‰ã€çˆ±ç‡ƒçƒ§ç­‰ï¼‰",
            "è¯·æœç´¢èµ›äº‹æ–°é—»æŠ¥é“ã€è·‘å‹åˆ†äº«ç­‰å¤šä¸ªæ¥æº"
        ]

        prompt_parts = [
            f"ã€é‡è¦ä»»åŠ¡ã€‘è¯·å¸®æˆ‘æœç´¢ã€Œ{event_name}ã€è¿™ä¸ªé©¬æ‹‰æ¾èµ›äº‹çš„è¯¦ç»†ä¿¡æ¯ã€‚",
            f"ã€æœç´¢å»ºè®®ã€‘{search_tips[min(retry, len(search_tips) - 1)]}",
            "",
            "ã€å¿…é¡»è¡¥å…¨çš„ä¿¡æ¯ã€‘"
        ]

        # åˆ—å‡ºç¼ºå¤±çš„å­—æ®µ
        if missing_info.get('missing_total_scale'):
            prompt_parts.append("âœ“ èµ›äº‹æ€»è§„æ¨¡/æ€»äººæ•°")

        if missing_info.get('incomplete_categories'):
            prompt_parts.append("âœ“ ä»¥ä¸‹ç»„åˆ«çš„è¯¦ç»†ä¿¡æ¯ï¼š")
            for item in missing_info['incomplete_categories']:
                cat = item['category']
                missing_fields = item['missing_fields']
                cat_name = cat.get('name', 'æœªçŸ¥ç»„åˆ«')
                prompt_parts.append(f"  Â· {cat_name}:")
                if 'name' in missing_fields:
                    prompt_parts.append(f"    - ç»„åˆ«åç§°")
                if 'distance' in missing_fields:
                    prompt_parts.append(f"    - è·ç¦»")
                if 'fee' in missing_fields:
                    prompt_parts.append(f"    - æŠ¥åè´¹ç”¨")
                if 'total_quota' in missing_fields:
                    prompt_parts.append(f"    - åé¢")

        prompt_parts.extend([
            "",
            "ã€è¿”å›æ ¼å¼ã€‘è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š",
            "{",
            '  "total_scale": "èµ›äº‹æ€»è§„æ¨¡ï¼ˆçº¯æ•°å­—ï¼Œå¦‚ï¼š10000ï¼‰",',
            '  "race_categories": [',
            '    {',
            '      "name": "ç»„åˆ«åç§°ï¼ˆå¦‚ï¼šå…¨ç¨‹é©¬æ‹‰æ¾ã€åŠç¨‹é©¬æ‹‰æ¾ï¼‰",',
            '      "distance": "è·ç¦»ï¼ˆæ ¼å¼ï¼š42.195kmã€21.0975kmï¼‰",',
            '      "fee": "æŠ¥åè´¹ç”¨ï¼ˆçº¯æ•°å­—ï¼Œå¦‚ï¼š120.00ï¼‰",',
            '      "total_quota": "åé¢ï¼ˆçº¯æ•°å­—ï¼Œå¦‚ï¼š5000ï¼‰"',
            '    }',
            '  ],',
            '  "source": "ä¿¡æ¯æ¥æºURLæˆ–åç§°",',
            '  "confidence": "high/medium/low"',
            "}",
            "",
            "ã€å…³é”®è¦æ±‚ã€‘",
            "1. å¿…é¡»æœç´¢çœŸå®çš„èµ›äº‹ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ",
            "2. æ‰€æœ‰æ•°å­—å¿…é¡»å‡†ç¡®ï¼ˆè·ç¦»ã€è´¹ç”¨ã€åé¢ï¼‰",
            "3. è·ç¦»æ ¼å¼ï¼š42.195kmã€21.0975kmã€10km",
            "4. è´¹ç”¨å’Œåé¢ï¼šçº¯æ•°å­—ï¼Œä¸å¸¦å•ä½",
            "5. å¦‚æœæŸä¸ªå­—æ®µç¡®å®æ‰¾ä¸åˆ°ï¼Œå¡«å†™null",
            "6. å¿…é¡»åˆ—å‡ºæ‰€æœ‰ç»„åˆ«ï¼ˆå…¨é©¬ã€åŠé©¬ã€è¿·ä½ ç­‰ï¼‰",
            "7. total_scale = æ‰€æœ‰ç»„åˆ«åé¢ä¹‹å’Œ"
        ])

        return '\n'.join(prompt_parts)

    # ==================== ç¬¬äºŒå±‚ï¼šç™¾åº¦æœç´¢API ====================

    def layer2_baidu_search(self, event_name: str, missing_info: Dict) -> Optional[Dict]:
        """ç¬¬äºŒå±‚ï¼šä½¿ç”¨ç™¾åº¦æœç´¢APIè·å–æœç´¢ç»“æœï¼Œç„¶åç”¨AIæå–"""
        try:
            logger.info(f"ğŸ” [ç¬¬2å±‚] ç™¾åº¦æœç´¢API")

            # æ„å»ºæœç´¢å…³é”®è¯
            search_queries = self._build_search_queries(event_name, missing_info)

            all_search_results = []

            for query in search_queries[:3]:  # æœ€å¤šæœç´¢3ä¸ªå…³é”®è¯
                logger.info(f"  æœç´¢å…³é”®è¯: {query}")

                # ä½¿ç”¨ç™¾åº¦æœç´¢ï¼ˆé€šè¿‡çˆ¬å–æœç´¢ç»“æœé¡µï¼‰
                search_results = self._baidu_search(query)

                if search_results:
                    all_search_results.extend(search_results)
                    logger.info(f"  è·å–åˆ° {len(search_results)} æ¡ç»“æœ")

                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

            if not all_search_results:
                logger.warning(f"âš  [ç¬¬2å±‚] æœªè·å–åˆ°æœç´¢ç»“æœ")
                return None

            # ä½¿ç”¨AIä»æœç´¢ç»“æœä¸­æå–ä¿¡æ¯
            logger.info(f"  ä½¿ç”¨AIåˆ†æ {len(all_search_results)} æ¡æœç´¢ç»“æœ")
            completed_data = self._extract_from_search_results(event_name, all_search_results, missing_info)

            if completed_data and self._validate_completed_data(completed_data, missing_info):
                logger.info(f"âœ“ [ç¬¬2å±‚] ç™¾åº¦æœç´¢æˆåŠŸ")
                return completed_data
            else:
                logger.warning(f"âš  [ç¬¬2å±‚] æ•°æ®æå–å¤±è´¥")
                return None

        except Exception as e:
            logger.error(f"âœ— [ç¬¬2å±‚] ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return None

    def _build_search_queries(self, event_name: str, missing_info: Dict) -> List[str]:
        """æ„å»ºæœç´¢å…³é”®è¯åˆ—è¡¨"""
        queries = [
            f"{event_name} æŠ¥åè´¹ç”¨ ç»„åˆ« åé¢",
            f"{event_name} å®˜æ–¹ æ‹›å‹Ÿ",
            f"{event_name} ç«èµ›è§„ç¨‹"
        ]
        return queries

    def _baidu_search(self, query: str, num_results: int = 5) -> List[Dict]:
        """ä½¿ç”¨ç™¾åº¦æœç´¢ï¼ˆçˆ¬å–æœç´¢ç»“æœé¡µï¼‰"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            url = f"https://www.baidu.com/s?wd={quote(query)}&rn={num_results}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            results = []
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            for item in soup.find_all('div', class_='result')[:num_results]:
                try:
                    # æå–æ ‡é¢˜
                    title_tag = item.find('h3') or item.find('a')
                    title = title_tag.get_text(strip=True) if title_tag else ''

                    # æå–é“¾æ¥
                    link_tag = item.find('a')
                    link = link_tag.get('href') if link_tag else ''

                    # æå–æ‘˜è¦
                    abstract_tag = item.find('div', class_='c-abstract') or item.find('span', class_='content-right_8Zs40')
                    abstract = abstract_tag.get_text(strip=True) if abstract_tag else ''

                    if title and abstract:
                        results.append({
                            'title': title,
                            'link': link,
                            'abstract': abstract
                        })
                except Exception as e:
                    logger.debug(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                    continue

            return results

        except Exception as e:
            logger.error(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []

    def _extract_from_search_results(self, event_name: str, search_results: List[Dict], missing_info: Dict) -> Optional[Dict]:
        """ä½¿ç”¨AIä»æœç´¢ç»“æœä¸­æå–ä¿¡æ¯"""
        try:
            from openai import OpenAI

            client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                http_client=httpx.Client(
                    base_url=self.api_base,
                    follow_redirects=True,
                )
            )

            # æ•´ç†æœç´¢ç»“æœæ–‡æœ¬
            search_text = f"èµ›äº‹åç§°ï¼š{event_name}\n\næœç´¢ç»“æœï¼š\n\n"
            for idx, result in enumerate(search_results[:10], 1):
                search_text += f"ã€ç»“æœ{idx}ã€‘\n"
                search_text += f"æ ‡é¢˜ï¼š{result['title']}\n"
                search_text += f"æ‘˜è¦ï¼š{result['abstract']}\n\n"

            prompt = f"""
è¯·ä»ä»¥ä¸‹æœç´¢ç»“æœä¸­æå–ã€Œ{event_name}ã€çš„èµ›äº‹ä¿¡æ¯ã€‚

{search_text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼ï¼š
{{
  "total_scale": "èµ›äº‹æ€»è§„æ¨¡ï¼ˆçº¯æ•°å­—ï¼‰",
  "race_categories": [
    {{
      "name": "ç»„åˆ«åç§°",
      "distance": "è·ç¦»ï¼ˆå¦‚ï¼š42.195kmï¼‰",
      "fee": "æŠ¥åè´¹ç”¨ï¼ˆçº¯æ•°å­—ï¼‰",
      "total_quota": "åé¢ï¼ˆçº¯æ•°å­—ï¼‰"
    }}
  ],
  "source": "ä¿¡æ¯æ¥æº",
  "confidence": "high/medium/low"
}}

è¦æ±‚ï¼š
1. åªæå–æœç´¢ç»“æœä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯
2. æ‰¾ä¸åˆ°çš„å­—æ®µå¡«å†™null
3. ç¡®ä¿æ•°å­—å‡†ç¡®
"""

            response = client.chat.completions.create(
                model=self.search_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ•°æ®æå–ä¸“å®¶ï¼Œä»æœç´¢ç»“æœä¸­æå–å‡†ç¡®çš„èµ›äº‹ä¿¡æ¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result = response.choices[0].message.content
            return json.loads(result)

        except Exception as e:
            logger.error(f"ä»æœç´¢ç»“æœæå–ä¿¡æ¯å¤±è´¥: {e}")
            return None

    # ==================== ç¬¬ä¸‰å±‚ï¼šå¾®ä¿¡å…¬ä¼—å·æœç´¢ ====================

    def layer3_wechat_search(self, event_name: str, missing_info: Dict) -> Optional[Dict]:
        """ç¬¬ä¸‰å±‚ï¼šæœç´¢å¾®ä¿¡å…¬ä¼—å·æ–‡ç« """
        try:
            logger.info(f"ğŸ” [ç¬¬3å±‚] å¾®ä¿¡å…¬ä¼—å·æœç´¢")

            # ä½¿ç”¨æœç‹—å¾®ä¿¡æœç´¢
            search_results = self._sogou_wechat_search(event_name)

            if not search_results:
                logger.warning(f"âš  [ç¬¬3å±‚] æœªæ‰¾åˆ°å…¬ä¼—å·æ–‡ç« ")
                return None

            logger.info(f"  æ‰¾åˆ° {len(search_results)} ç¯‡å…¬ä¼—å·æ–‡ç« ")

            # ä½¿ç”¨AIä»æ–‡ç« æ‘˜è¦ä¸­æå–ä¿¡æ¯
            completed_data = self._extract_from_search_results(event_name, search_results, missing_info)

            if completed_data and self._validate_completed_data(completed_data, missing_info):
                logger.info(f"âœ“ [ç¬¬3å±‚] å…¬ä¼—å·æœç´¢æˆåŠŸ")
                return completed_data
            else:
                logger.warning(f"âš  [ç¬¬3å±‚] æ•°æ®æå–å¤±è´¥")
                return None

        except Exception as e:
            logger.error(f"âœ— [ç¬¬3å±‚] å…¬ä¼—å·æœç´¢å¤±è´¥: {e}")
            return None

    def _sogou_wechat_search(self, query: str, num_results: int = 5) -> List[Dict]:
        """ä½¿ç”¨æœç‹—å¾®ä¿¡æœç´¢"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            url = f"https://weixin.sogou.com/weixin?type=2&query={quote(query)}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            results = []
            for item in soup.find_all('div', class_='txt-box')[:num_results]:
                try:
                    title_tag = item.find('h3') or item.find('a')
                    title = title_tag.get_text(strip=True) if title_tag else ''

                    abstract_tag = item.find('p', class_='txt-info')
                    abstract = abstract_tag.get_text(strip=True) if abstract_tag else ''

                    if title and abstract:
                        results.append({
                            'title': title,
                            'abstract': abstract,
                            'link': ''
                        })
                except Exception as e:
                    logger.debug(f"è§£æå…¬ä¼—å·ç»“æœå¤±è´¥: {e}")
                    continue

            return results

        except Exception as e:
            logger.error(f"æœç‹—å¾®ä¿¡æœç´¢å¤±è´¥: {e}")
            return []

    # ==================== æ•°æ®éªŒè¯å’Œåˆå¹¶ ====================

    def _validate_completed_data(self, completed_data: Dict, missing_info: Dict) -> bool:
        """éªŒè¯è¡¥å…¨æ•°æ®çš„è´¨é‡"""
        if not completed_data:
            return False

        # æ£€æŸ¥æ˜¯å¦è‡³å°‘è¡¥å…¨äº†ä¸€äº›å…³é”®ä¿¡æ¯
        has_valid_data = False

        # æ£€æŸ¥æ€»è§„æ¨¡
        if missing_info.get('missing_total_scale'):
            if completed_data.get('total_scale') and completed_data['total_scale'] != 'null':
                has_valid_data = True

        # æ£€æŸ¥ç»„åˆ«ä¿¡æ¯
        if completed_data.get('race_categories'):
            for cat in completed_data['race_categories']:
                if (cat.get('name') and cat['name'] != 'null' and
                    cat.get('distance') and cat['distance'] != 'null' and
                    cat.get('fee') and cat['fee'] != 'null'):
                    has_valid_data = True
                    break

        return has_valid_data

    def merge_completed_data(self, original_data: Dict, completed_data: Dict) -> Dict:
        """åˆå¹¶åŸå§‹æ•°æ®å’Œè¡¥å…¨æ•°æ®"""
        if not completed_data:
            return original_data

        merged = original_data.copy()

        # è¡¥å…¨èµ›äº‹æ€»è§„æ¨¡
        if completed_data.get('total_scale') and completed_data['total_scale'] != 'null':
            if not merged.get('total_scale') or merged.get('total_scale') == 'null':
                merged['total_scale'] = completed_data['total_scale']
                logger.info(f"  âœ“ è¡¥å…¨èµ›äº‹æ€»è§„æ¨¡: {completed_data['total_scale']}")

        # è¡¥å…¨ç»„åˆ«ä¿¡æ¯
        if completed_data.get('race_categories'):
            if not merged.get('race_categories') or len(merged['race_categories']) == 0:
                merged['race_categories'] = completed_data['race_categories']
                logger.info(f"  âœ“ ä½¿ç”¨è¡¥å…¨æ•°æ®å¡«å……ç»„åˆ«ä¿¡æ¯ï¼ˆå…± {len(completed_data['race_categories'])} ä¸ªç»„åˆ«ï¼‰")
            else:
                # æ™ºèƒ½åˆå¹¶
                original_categories = merged['race_categories']
                completed_categories = completed_data['race_categories']

                for orig_cat in original_categories:
                    orig_name = orig_cat.get('name', '').strip()

                    for comp_cat in completed_categories:
                        comp_name = comp_cat.get('name', '').strip()

                        if orig_name and comp_name and self._is_similar_category_name(orig_name, comp_name):
                            # è¡¥å…¨ç¼ºå¤±å­—æ®µ
                            for field in ['distance', 'fee', 'total_quota']:
                                if (not orig_cat.get(field) or orig_cat.get(field) == 'null') and comp_cat.get(field) and comp_cat.get(field) != 'null':
                                    orig_cat[field] = comp_cat[field]
                                    logger.info(f"  âœ“ è¡¥å…¨ {orig_name} çš„ {field}: {comp_cat[field]}")
                            break

                merged['race_categories'] = original_categories

        # è®°å½•è¡¥å…¨æ¥æº
        if completed_data.get('source'):
            merged['completion_source'] = completed_data['source']
        if completed_data.get('confidence'):
            merged['completion_confidence'] = completed_data['confidence']

        return merged

    def _is_similar_category_name(self, name1: str, name2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªç»„åˆ«åç§°æ˜¯å¦ç›¸ä¼¼"""
        keywords = ['å…¨ç¨‹', 'åŠç¨‹', 'é©¬æ‹‰æ¾', 'è¿·ä½ ', 'å¥åº·è·‘', 'äº²å­è·‘', '10å…¬é‡Œ', '5å…¬é‡Œ', '10km', '5km']
        name1_lower = name1.lower()
        name2_lower = name2.lower()

        for keyword in keywords:
            if keyword in name1_lower and keyword in name2_lower:
                return True
        return False

    # ==================== ä¸»è¡¥å…¨å‡½æ•° ====================

    def complete_event_data(self, event_name: str, structured_data: Dict) -> Dict:
        """å¤šå±‚ç­–ç•¥è¡¥å…¨èµ›äº‹æ•°æ®"""
        logger.info(f"\nã€æ•°æ®è¡¥å…¨ã€‘æ£€æŸ¥èµ›äº‹: {event_name}")
        logger.info("-" * 80)

        # æ£€æŸ¥ç¼ºå¤±å­—æ®µ
        missing_info = self.check_missing_fields(structured_data)

        if not missing_info['has_missing']:
            logger.info("âœ“ æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
            return structured_data

        logger.info(f"âš  å‘ç°ç¼ºå¤±å­—æ®µï¼Œå¯åŠ¨å¤šå±‚è¡¥å…¨ç­–ç•¥...")

        # ç¬¬ä¸€å±‚ï¼šAIè”ç½‘æœç´¢ï¼ˆå¸¦é‡è¯•ï¼‰
        for retry in range(self.max_retries):
            completed_data = self.layer1_ai_search(event_name, missing_info, retry)
            if completed_data:
                merged_data = self.merge_completed_data(structured_data, completed_data)
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç¼ºå¤±
                final_missing = self.check_missing_fields(merged_data)
                if not final_missing['has_missing']:
                    logger.info("âœ“âœ“âœ“ [ç¬¬1å±‚] æ•°æ®è¡¥å…¨æˆåŠŸ")
                    return merged_data
                else:
                    logger.info(f"âš  [ç¬¬1å±‚] éƒ¨åˆ†å­—æ®µä»ç¼ºå¤±ï¼Œç»§ç»­é‡è¯•...")
                    missing_info = final_missing
                    structured_data = merged_data

        # ç¬¬äºŒå±‚ï¼šç™¾åº¦æœç´¢API
        logger.info(f"â†’ [ç¬¬1å±‚] æœªå®Œå…¨è¡¥å…¨ï¼Œå°è¯•ç¬¬2å±‚...")
        completed_data = self.layer2_baidu_search(event_name, missing_info)
        if completed_data:
            merged_data = self.merge_completed_data(structured_data, completed_data)
            final_missing = self.check_missing_fields(merged_data)
            if not final_missing['has_missing']:
                logger.info("âœ“âœ“âœ“ [ç¬¬2å±‚] æ•°æ®è¡¥å…¨æˆåŠŸ")
                return merged_data
            else:
                logger.info(f"âš  [ç¬¬2å±‚] éƒ¨åˆ†å­—æ®µä»ç¼ºå¤±ï¼Œç»§ç»­å°è¯•...")
                missing_info = final_missing
                structured_data = merged_data

        # ç¬¬ä¸‰å±‚ï¼šå¾®ä¿¡å…¬ä¼—å·æœç´¢
        logger.info(f"â†’ [ç¬¬2å±‚] æœªå®Œå…¨è¡¥å…¨ï¼Œå°è¯•ç¬¬3å±‚...")
        completed_data = self.layer3_wechat_search(event_name, missing_info)
        if completed_data:
            merged_data = self.merge_completed_data(structured_data, completed_data)
            final_missing = self.check_missing_fields(merged_data)
            if not final_missing['has_missing']:
                logger.info("âœ“âœ“âœ“ [ç¬¬3å±‚] æ•°æ®è¡¥å…¨æˆåŠŸ")
                return merged_data
            else:
                logger.warning("âš  [ç¬¬3å±‚] éƒ¨åˆ†å­—æ®µä»ç¼ºå¤±")
                return merged_data

        logger.warning("âš  æ‰€æœ‰è¡¥å…¨ç­–ç•¥å·²å°è¯•ï¼Œè¿”å›å½“å‰æ•°æ®")
        return structured_data