"""
æ™ºèƒ½æ•°æ®è¡¥å…¨å™¨ - ç”Ÿäº§çº§å¤šæºéªŒè¯ç³»ç»Ÿ
ç‰¹æ€§:
1. å¤šæ•°æ®æºè¡¥å…¨(é©¬æ‹‰æ¾ä¸“ä¸šå¹³å°ã€å®˜æ–¹æŠ¥åç³»ç»Ÿç­‰)
2. æ•°æ®ç½®ä¿¡åº¦è¯„åˆ†
3. å†å²æ•°æ®æ™ºèƒ½æ¨æ–­
4. å¤šæºäº¤å‰éªŒè¯
5. å¼‚å¸¸å€¼æ£€æµ‹
"""
import logging
from typing import Dict, List, Optional, Tuple
from config import QWEN_API_KEY, QWEN_API_BASE
import httpx
import json
import requests
import time
import re
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin
from datetime import datetime
from collections import defaultdict
import statistics

logger = logging.getLogger(__name__)


class SmartDataCompleter:
    """æ™ºèƒ½æ•°æ®è¡¥å…¨å™¨"""

    def __init__(self):
        self.api_key = QWEN_API_KEY
        self.api_base = QWEN_API_BASE
        self.search_model = "qwen-plus"

        # æ•°æ®æºæƒé‡é…ç½®(ç”¨äºç½®ä¿¡åº¦è®¡ç®—)
        self.source_weights = {
            'official_website': 1.0,  # å®˜æ–¹ç½‘ç«™
            'registration_platform': 0.95,  # æŠ¥åå¹³å°
            'marathon_media': 0.85,  # é©¬æ‹‰æ¾åª’ä½“
            'wechat_official': 0.90,  # å®˜æ–¹å…¬ä¼—å·
            'ai_search': 0.70,  # AIæœç´¢
            'inference': 0.60,  # æ¨æ–­æ•°æ®
        }

        # é©¬æ‹‰æ¾æ ‡å‡†é…ç½®(ç”¨äºæ¨æ–­å’ŒéªŒè¯)
        self.standard_races = {
            'å…¨ç¨‹é©¬æ‹‰æ¾': {'distance': '42.195km', 'keywords': ['å…¨é©¬', 'å…¨ç¨‹', 'marathon', '42']},
            'åŠç¨‹é©¬æ‹‰æ¾': {'distance': '21.0975km', 'keywords': ['åŠé©¬', 'åŠç¨‹', 'half', '21']},
            '10å…¬é‡Œ': {'distance': '10km', 'keywords': ['10å…¬é‡Œ', '10km', '10k']},
            '5å…¬é‡Œ': {'distance': '5km', 'keywords': ['5å…¬é‡Œ', '5km', '5k']},
            'è¿·ä½ é©¬æ‹‰æ¾': {'distance': '5km', 'keywords': ['è¿·ä½ ', 'mini']},
            'å¥åº·è·‘': {'distance': '3km', 'keywords': ['å¥åº·è·‘', 'å¥åº·']},
        }

        # å†å²æ•°æ®ç¼“å­˜(ç”¨äºæ¨æ–­)
        self.historical_data = {}

    # ==================== æ ¸å¿ƒè¡¥å…¨æµç¨‹ ====================

    def complete_event_data(self, event_name: str, structured_data: Dict) -> Dict:
        """æ™ºèƒ½è¡¥å…¨èµ›äº‹æ•°æ® - ä¸»å…¥å£"""
        logger.info(f"\n{'=' * 80}")
        logger.info(f"ğŸ¯ æ™ºèƒ½æ•°æ®è¡¥å…¨: {event_name}")
        logger.info(f"{'=' * 80}")

        # æ­¥éª¤1: æ£€æŸ¥ç¼ºå¤±å­—æ®µ
        missing_info = self.check_missing_fields(structured_data)

        if not missing_info['has_missing']:
            logger.info("âœ“ æ•°æ®å®Œæ•´,æ— éœ€è¡¥å…¨")
            return structured_data

        logger.info(f"\nğŸ“‹ ç¼ºå¤±å­—æ®µåˆ†æ:")
        logger.info(f"  - èµ›äº‹æ€»è§„æ¨¡: {'ç¼ºå¤±' if missing_info['missing_total_scale'] else 'å®Œæ•´'}")
        logger.info(f"  - ä¸å®Œæ•´ç»„åˆ«æ•°: {len(missing_info['incomplete_categories'])}")

        # æ­¥éª¤2: å¤šæºæ•°æ®æ”¶é›†
        logger.info(f"\nğŸ” å¯åŠ¨å¤šæºæ•°æ®æ”¶é›†...")
        multi_source_data = self.collect_from_multiple_sources(event_name, missing_info)

        # æ­¥éª¤3: æ•°æ®èåˆä¸éªŒè¯
        logger.info(f"\nğŸ”¬ æ•°æ®èåˆä¸äº¤å‰éªŒè¯...")
        validated_data = self.validate_and_merge(multi_source_data, missing_info)

        # æ­¥éª¤4: æ™ºèƒ½æ¨æ–­è¡¥å……
        logger.info(f"\nğŸ§  æ™ºèƒ½æ¨æ–­è¡¥å……...")
        final_data = self.intelligent_inference(structured_data, validated_data, event_name)

        # æ­¥éª¤5: åˆå¹¶åˆ°åŸå§‹æ•°æ®
        logger.info(f"\nğŸ“Š åˆå¹¶æ•°æ®...")
        result = self.merge_completed_data(structured_data, final_data)

        # æ­¥éª¤6: æœ€ç»ˆéªŒè¯
        final_missing = self.check_missing_fields(result)

        completion_rate = self.calculate_completion_rate(missing_info, final_missing)
        logger.info(f"\nâœ… è¡¥å…¨å®Œæˆç‡: {completion_rate:.1f}%")

        if final_missing['has_missing']:
            logger.warning(f"âš  ä»æœ‰éƒ¨åˆ†å­—æ®µç¼ºå¤±")
        else:
            logger.info(f"âœ“âœ“âœ“ æ‰€æœ‰å…³é”®å­—æ®µå·²å®Œæ•´è¡¥å…¨")

        return result

    # ==================== å¤šæºæ•°æ®æ”¶é›† ====================

    def collect_from_multiple_sources(self, event_name: str, missing_info: Dict) -> List[Dict]:
        """ä»å¤šä¸ªæ•°æ®æºæ”¶é›†ä¿¡æ¯"""
        sources_data = []

        # æ•°æ®æº1: AIè”ç½‘æœç´¢(é€šä¹‰åƒé—®)
        logger.info(f"  [æº1] AIè”ç½‘æœç´¢...")
        ai_data = self.source1_ai_search(event_name, missing_info)
        if ai_data:
            ai_data['source_type'] = 'ai_search'
            ai_data['confidence_base'] = self.source_weights['ai_search']
            sources_data.append(ai_data)
            logger.info(f"    âœ“ è·å–åˆ°æ•°æ®")

        # æ•°æ®æº2: é©¬æ‹‰æ¾ä¸“ä¸šå¹³å°æœç´¢
        logger.info(f"  [æº2] é©¬æ‹‰æ¾å¹³å°æœç´¢...")
        platform_data = self.source2_marathon_platforms(event_name, missing_info)
        if platform_data:
            platform_data['source_type'] = 'registration_platform'
            platform_data['confidence_base'] = self.source_weights['registration_platform']
            sources_data.append(platform_data)
            logger.info(f"    âœ“ è·å–åˆ°æ•°æ®")

        # æ•°æ®æº3: ç™¾åº¦ç²¾å‡†æœç´¢
        logger.info(f"  [æº3] ç™¾åº¦ç²¾å‡†æœç´¢...")
        baidu_data = self.source3_baidu_precise_search(event_name, missing_info)
        if baidu_data:
            baidu_data['source_type'] = 'marathon_media'
            baidu_data['confidence_base'] = self.source_weights['marathon_media']
            sources_data.append(baidu_data)
            logger.info(f"    âœ“ è·å–åˆ°æ•°æ®")

        # æ•°æ®æº4: å¾®ä¿¡å…¬ä¼—å·æ·±åº¦æœç´¢
        logger.info(f"  [æº4] å¾®ä¿¡å…¬ä¼—å·æœç´¢...")
        wechat_data = self.source4_wechat_deep_search(event_name, missing_info)
        if wechat_data:
            wechat_data['source_type'] = 'wechat_official'
            wechat_data['confidence_base'] = self.source_weights['wechat_official']
            sources_data.append(wechat_data)
            logger.info(f"    âœ“ è·å–åˆ°æ•°æ®")

        logger.info(f"\n  ğŸ“Š å…±æ”¶é›†åˆ° {len(sources_data)} ä¸ªæ•°æ®æºçš„ä¿¡æ¯")
        return sources_data

    def source1_ai_search(self, event_name: str, missing_info: Dict) -> Optional[Dict]:
        """æ•°æ®æº1: AIè”ç½‘æœç´¢"""
        try:
            from openai import OpenAI

            client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                http_client=httpx.Client(
                    base_url=self.api_base,
                    follow_redirects=True,
                )
            )

            prompt = self._build_precise_search_prompt(event_name, missing_info)

            response = client.chat.completions.create(
                model=self.search_model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸“ä¸šçš„é©¬æ‹‰æ¾èµ›äº‹æ•°æ®æ”¶é›†ä¸“å®¶ã€‚ä½ å¿…é¡»:
1. é€šè¿‡æœç´¢å¼•æ“æŸ¥æ‰¾çœŸå®å‡†ç¡®çš„èµ›äº‹ä¿¡æ¯
2. ä¼˜å…ˆä»å®˜æ–¹æ¸ é“è·å–(å®˜ç½‘ã€å®˜æ–¹å…¬ä¼—å·ã€å®˜æ–¹æŠ¥åå¹³å°)
3. è¿”å›çš„æ•°æ®å¿…é¡»å‡†ç¡®,ä¸èƒ½ç¼–é€ 
4. å¦‚æœæ‰¾ä¸åˆ°æŸä¸ªå­—æ®µ,å¡«å†™nullè€Œä¸æ˜¯çŒœæµ‹
5. æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºå’Œå¯ä¿¡åº¦"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)

            # éªŒè¯æ•°æ®åˆç†æ€§
            if self._validate_ai_response(result, event_name):
                return result
            else:
                logger.warning("    AIè¿”å›æ•°æ®æœªé€šè¿‡éªŒè¯")
                return None

        except Exception as e:
            logger.error(f"    AIæœç´¢å¤±è´¥: {e}")
            return None

    def source2_marathon_platforms(self, event_name: str, missing_info: Dict) -> Optional[Dict]:
        """æ•°æ®æº2: é©¬æ‹‰æ¾ä¸“ä¸šå¹³å°(æœ€é…·ã€é©¬æ‹‰é©¬æ‹‰ã€çˆ±ç‡ƒçƒ§ç­‰)"""
        try:
            # æ„å»ºæœç´¢å…³é”®è¯
            keywords = [
                f"{event_name} æŠ¥å",
                f"{event_name} ç»„åˆ«",
                f"{event_name} è´¹ç”¨"
            ]

            all_results = []

            # æœç´¢é©¬æ‹‰æ¾å¹³å°
            platforms = [
                'zuicool.com',
                'malama.com.cn',
                'iranshao.com',
                'marathon.org.cn'
            ]

            for platform in platforms:
                for keyword in keywords[:2]:  # æ¯ä¸ªå¹³å°æœç´¢2ä¸ªå…³é”®è¯
                    query = f"site:{platform} {keyword}"
                    results = self._baidu_search(query, num=3)
                    all_results.extend(results)
                    time.sleep(1)

            if not all_results:
                return None

            # ä½¿ç”¨AIä»æœç´¢ç»“æœä¸­æå–ä¿¡æ¯
            extracted = self._extract_from_search_results(
                event_name,
                all_results,
                missing_info,
                source_hint="é©¬æ‹‰æ¾ä¸“ä¸šå¹³å°"
            )

            return extracted

        except Exception as e:
            logger.error(f"    é©¬æ‹‰æ¾å¹³å°æœç´¢å¤±è´¥: {e}")
            return None

    def source3_baidu_precise_search(self, event_name: str, missing_info: Dict) -> Optional[Dict]:
        """æ•°æ®æº3: ç™¾åº¦ç²¾å‡†æœç´¢"""
        try:
            # æ„å»ºé«˜ç²¾åº¦æœç´¢æŸ¥è¯¢
            queries = []

            if missing_info.get('missing_total_scale'):
                queries.append(f'"{event_name}" èµ›äº‹è§„æ¨¡ äººæ•°')
                queries.append(f'"{event_name}" æ€»åé¢')

            if missing_info.get('incomplete_categories'):
                queries.append(f'"{event_name}" ç»„åˆ« è·ç¦» è´¹ç”¨')
                queries.append(f'"{event_name}" æŠ¥åè´¹ å…¨é©¬ åŠé©¬')

            all_results = []
            for query in queries[:3]:
                results = self._baidu_search(query, num=5)
                all_results.extend(results)
                time.sleep(1.5)

            if not all_results:
                return None

            # æå–ä¿¡æ¯
            extracted = self._extract_from_search_results(
                event_name,
                all_results,
                missing_info,
                source_hint="ç™¾åº¦æœç´¢"
            )

            return extracted

        except Exception as e:
            logger.error(f"    ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return None

    def source4_wechat_deep_search(self, event_name: str, missing_info: Dict) -> Optional[Dict]:
        """æ•°æ®æº4: å¾®ä¿¡å…¬ä¼—å·æ·±åº¦æœç´¢"""
        try:
            # ä½¿ç”¨æœç‹—å¾®ä¿¡æœç´¢
            search_queries = [
                f"{event_name} æŠ¥å",
                f"{event_name} ç«èµ›è§„ç¨‹",
                f"{event_name} æ‹›å‹Ÿ"
            ]

            all_articles = []
            for query in search_queries[:2]:
                articles = self._sogou_wechat_search(query, num=5)
                all_articles.extend(articles)
                time.sleep(1)

            if not all_articles:
                return None

            # ä»å…¬ä¼—å·æ–‡ç« ä¸­æå–ä¿¡æ¯
            extracted = self._extract_from_search_results(
                event_name,
                all_articles,
                missing_info,
                source_hint="å¾®ä¿¡å…¬ä¼—å·"
            )
            return extracted

        except Exception as e:
            logger.error(f"    å¾®ä¿¡æœç´¢å¤±è´¥: {e}")
            return None

    # ==================== æ•°æ®éªŒè¯ä¸èåˆ ====================

    def validate_and_merge(self, sources_data: List[Dict], missing_info: Dict) -> Dict:
        """éªŒè¯å¹¶èåˆå¤šæºæ•°æ®"""
        if not sources_data:
            return {}

        logger.info(f"  ğŸ”¬ å¼€å§‹äº¤å‰éªŒè¯...")

        # å¯¹æ¯ä¸ªå­—æ®µè¿›è¡Œå¤šæºéªŒè¯
        merged = {
            'total_scale': None,
            'race_categories': [],
            'sources_used': [],
            'confidence_scores': {}
        }

        # éªŒè¯æ€»è§„æ¨¡
        total_scales = []
        for source in sources_data:
            if source.get('total_scale') and source['total_scale'] != 'null':
                try:
                    scale = int(re.sub(r'[^\d]', '', str(source['total_scale'])))
                    if 100 <= scale <= 100000:  # åˆç†èŒƒå›´
                        total_scales.append({
                            'value': scale,
                            'source': source.get('source'),
                            'confidence': source.get('confidence_base', 0.5)
                        })
                except:
                    pass

        if total_scales:
            # ä½¿ç”¨åŠ æƒå¹³å‡å’Œç½®ä¿¡åº¦
            merged['total_scale'] = self._weighted_consensus(total_scales)
            merged['confidence_scores']['total_scale'] = self._calculate_field_confidence(total_scales)
            logger.info(
                f"    âœ“ æ€»è§„æ¨¡: {merged['total_scale']} (ç½®ä¿¡åº¦: {merged['confidence_scores']['total_scale']:.2f})")

        # éªŒè¯ç»„åˆ«ä¿¡æ¯
        all_categories = []
        for source in sources_data:
            if source.get('race_categories'):
                for cat in source['race_categories']:
                    if self._is_valid_category(cat):
                        cat['source'] = source.get('source')
                        cat['confidence'] = source.get('confidence_base', 0.5)
                        all_categories.append(cat)

        if all_categories:
            # æŒ‰ç»„åˆ«åç§°åˆ†ç»„å¹¶éªŒè¯
            merged['race_categories'] = self._merge_categories_with_validation(all_categories)
            logger.info(f"    âœ“ ç»„åˆ«: {len(merged['race_categories'])} ä¸ª")

        # è®°å½•ä½¿ç”¨çš„æ•°æ®æº
        merged['sources_used'] = list(set([s.get('source', 'unknown') for s in sources_data if s.get('source')]))
        merged['overall_confidence'] = sum(merged['confidence_scores'].values()) / len(merged['confidence_scores']) if \
        merged['confidence_scores'] else 0.5

        logger.info(f"    ğŸ“Š æ•´ä½“ç½®ä¿¡åº¦: {merged['overall_confidence']:.2f}")

        return merged

    def _merge_categories_with_validation(self, categories: List[Dict]) -> List[Dict]:
        """åˆå¹¶å¹¶éªŒè¯ç»„åˆ«æ•°æ®"""
        # æŒ‰ç»„åˆ«åç§°åˆ†ç»„
        grouped = defaultdict(list)
        for cat in categories:
            name = self._normalize_category_name(cat.get('name', ''))
            if name:
                grouped[name].append(cat)

        merged_cats = []
        for name, cats in grouped.items():
            if len(cats) == 1:
                # åªæœ‰ä¸€ä¸ªæ¥æº
                merged_cats.append(cats[0])
            else:
                # å¤šä¸ªæ¥æº,éœ€è¦éªŒè¯å’Œèåˆ
                merged_cat = self._merge_single_category(cats, name)
                if merged_cat:
                    merged_cats.append(merged_cat)

        return merged_cats

    def _merge_single_category(self, categories: List[Dict], name: str) -> Optional[Dict]:
        """èåˆå•ä¸ªç»„åˆ«çš„å¤šæºæ•°æ®"""
        merged = {'name': name}

        # è·ç¦»éªŒè¯
        distances = [c.get('distance') for c in categories if c.get('distance') and c['distance'] != 'null']
        if distances:
            # æ£€æŸ¥ä¸€è‡´æ€§
            unique_distances = list(set(distances))
            if len(unique_distances) == 1:
                merged['distance'] = unique_distances[0]
                merged['distance_confidence'] = 1.0
            else:
                # æœ‰åˆ†æ­§,ä½¿ç”¨å‡ºç°æ¬¡æ•°æœ€å¤šçš„
                merged['distance'] = max(set(distances), key=distances.count)
                merged['distance_confidence'] = distances.count(merged['distance']) / len(distances)

        # è´¹ç”¨éªŒè¯(å–ä¸­ä½æ•°é¿å…å¼‚å¸¸å€¼)
        fees = []
        for c in categories:
            if c.get('fee') and c['fee'] != 'null':
                try:
                    fee = float(re.sub(r'[^\d.]', '', str(c['fee'])))
                    if 0 < fee < 1000:  # åˆç†èŒƒå›´
                        fees.append(fee)
                except:
                    pass

        if fees:
            if len(fees) == 1:
                merged['fee'] = str(fees[0])
                merged['fee_confidence'] = 1.0
            else:
                # ä½¿ç”¨ä¸­ä½æ•°
                median_fee = statistics.median(fees)
                merged['fee'] = str(median_fee)
                # è®¡ç®—ç½®ä¿¡åº¦(è´¹ç”¨å·®å¼‚å°äº20%è®¤ä¸ºä¸€è‡´)
                variations = [abs(f - median_fee) / median_fee for f in fees]
                merged['fee_confidence'] = 1.0 - (sum(variations) / len(variations))

        # åé¢éªŒè¯
        quotas = []
        for c in categories:
            if c.get('total_quota') and c['total_quota'] != 'null':
                try:
                    quota = int(re.sub(r'[^\d]', '', str(c['total_quota'])))
                    if 10 <= quota <= 50000:
                        quotas.append(quota)
                except:
                    pass

        if quotas:
            if len(quotas) == 1:
                merged['total_quota'] = str(quotas[0])
                merged['quota_confidence'] = 1.0
            else:
                # ä½¿ç”¨ä¸­ä½æ•°
                median_quota = int(statistics.median(quotas))
                merged['total_quota'] = str(median_quota)
                # è®¡ç®—ç½®ä¿¡åº¦
                variations = [abs(q - median_quota) / median_quota for q in quotas]
                merged['quota_confidence'] = 1.0 - (sum(variations) / len(variations))

        # è®¡ç®—ç»„åˆ«æ•´ä½“ç½®ä¿¡åº¦
        confidences = []
        for field in ['distance_confidence', 'fee_confidence', 'quota_confidence']:
            if field in merged:
                confidences.append(merged[field])

        if confidences:
            merged['overall_confidence'] = sum(confidences) / len(confidences)

        return merged if len(merged) > 1 else None

    # ==================== æ™ºèƒ½æ¨æ–­ ====================

    def intelligent_inference(self, original_data: Dict, validated_data: Dict, event_name: str) -> Dict:
        """æ™ºèƒ½æ¨æ–­ç¼ºå¤±æ•°æ®"""
        result = validated_data.copy()

        # æ¨æ–­1: åŸºäºèµ›äº‹åç§°æ¨æ–­ç»„åˆ«å’Œè·ç¦»
        if not result.get('race_categories') or len(result['race_categories']) == 0:
            logger.info("  ğŸ§  åŸºäºèµ›äº‹åç§°æ¨æ–­ç»„åˆ«...")
            inferred_cats = self._infer_categories_from_name(event_name)
            if inferred_cats:
                for cat in inferred_cats:
                    cat['inferred'] = True
                    cat['confidence'] = self.source_weights['inference']
                result['race_categories'] = inferred_cats
                logger.info(f"    âœ“ æ¨æ–­å‡º {len(inferred_cats)} ä¸ªç»„åˆ«")

        # æ¨æ–­2: è¡¥å…¨ç»„åˆ«çš„è·ç¦»
        if result.get('race_categories'):
            for cat in result['race_categories']:
                if not cat.get('distance') or cat['distance'] == 'null':
                    inferred_distance = self._infer_distance_from_name(cat.get('name', ''))
                    if inferred_distance:
                        cat['distance'] = inferred_distance
                        cat['distance_inferred'] = True
                        logger.info(f"    âœ“ æ¨æ–­ {cat['name']} è·ç¦»: {inferred_distance}")

        # æ¨æ–­3: åŸºäºå†å²æ•°æ®æ¨æ–­è´¹ç”¨èŒƒå›´
        if result.get('race_categories'):
            for cat in result['race_categories']:
                if not cat.get('fee') or cat['fee'] == 'null':
                    inferred_fee = self._infer_fee_from_history(cat.get('name', ''), event_name)
                    if inferred_fee:
                        cat['fee'] = inferred_fee
                        cat['fee_inferred'] = True
                        logger.info(f"    âœ“ æ¨æ–­ {cat['name']} è´¹ç”¨: {inferred_fee}")

        # æ¨æ–­4: è®¡ç®—æ€»è§„æ¨¡(å¦‚æœç¼ºå¤±)
        if not result.get('total_scale') or result['total_scale'] == 'null':
            if result.get('race_categories'):
                total = 0
                for cat in result['race_categories']:
                    if cat.get('total_quota') and cat['total_quota'] != 'null':
                        try:
                            quota = int(re.sub(r'[^\d]', '', str(cat['total_quota'])))
                            total += quota
                        except:
                            pass

                if total > 0:
                    result['total_scale'] = str(total)
                    result['total_scale_inferred'] = True
                    logger.info(f"    âœ“ æ¨æ–­æ€»è§„æ¨¡: {total}")

        return result

    def _infer_categories_from_name(self, event_name: str) -> List[Dict]:
        """ä»èµ›äº‹åç§°æ¨æ–­ç»„åˆ«"""
        categories = []
        name_lower = event_name.lower()

        # æ£€æŸ¥åŒ…å«å“ªäº›ç»„åˆ«å…³é”®è¯
        for race_name, config in self.standard_races.items():
            for keyword in config['keywords']:
                if keyword in name_lower:
                    categories.append({
                        'name': race_name,
                        'distance': config['distance'],
                        'fee': 'null',
                        'total_quota': 'null'
                    })
                    break

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°,é»˜è®¤æ¨æ–­å¸¸è§ç»„åˆ«
        if not categories:
            if 'é©¬æ‹‰æ¾' in event_name or 'marathon' in name_lower:
                categories = [
                    {'name': 'å…¨ç¨‹é©¬æ‹‰æ¾', 'distance': '42.195km', 'fee': 'null', 'total_quota': 'null'},
                    {'name': 'åŠç¨‹é©¬æ‹‰æ¾', 'distance': '21.0975km', 'fee': 'null', 'total_quota': 'null'},
                ]

        return categories

    def _infer_distance_from_name(self, category_name: str) -> Optional[str]:
        """ä»ç»„åˆ«åç§°æ¨æ–­è·ç¦»"""
        if not category_name:
            return None

        name_lower = category_name.lower()

        for race_name, config in self.standard_races.items():
            for keyword in config['keywords']:
                if keyword in name_lower:
                    return config['distance']

        # å°è¯•ä»åç§°ä¸­æå–æ•°å­—
        distance_match = re.search(r'(\d+\.?\d*)\s*(å…¬é‡Œ|km|k)', name_lower)
        if distance_match:
            number = distance_match.group(1)
            return f"{number}km"

        return None

    def _infer_fee_from_history(self, category_name: str, event_name: str) -> Optional[str]:
        """åŸºäºå†å²æ•°æ®æ¨æ–­è´¹ç”¨"""
        # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“æŸ¥è¯¢å†å²æ•°æ®
        # æš‚æ—¶ä½¿ç”¨ç»éªŒå€¼

        fee_ranges = {
            'å…¨ç¨‹é©¬æ‹‰æ¾': (100, 180),
            'åŠç¨‹é©¬æ‹‰æ¾': (80, 150),
            '10å…¬é‡Œ': (60, 100),
            '5å…¬é‡Œ': (50, 80),
            'è¿·ä½ é©¬æ‹‰æ¾': (50, 80),
        }

        for race_type, (min_fee, max_fee) in fee_ranges.items():
            if race_type in category_name:
                # è¿”å›ä¸­é—´å€¼
                avg_fee = (min_fee + max_fee) / 2
                return f"{avg_fee:.2f}"

        return None

    # ==================== è¾…åŠ©å‡½æ•° ====================

    def check_missing_fields(self, structured_data: Dict) -> Dict:
        """æ£€æŸ¥ç¼ºå¤±å­—æ®µ"""
        missing = {
            'has_missing': False,
            'missing_fields': [],
            'incomplete_categories': [],
            'missing_total_scale': False
        }

        # æ£€æŸ¥æ€»è§„æ¨¡
        total_scale = structured_data.get('total_scale')
        if not total_scale or total_scale == 'null' or str(total_scale).strip() == '':
            missing['has_missing'] = True
            missing['missing_total_scale'] = True
            missing['missing_fields'].append('total_scale')

        # æ£€æŸ¥ç»„åˆ«
        race_categories = structured_data.get('race_categories', [])
        if not race_categories:
            missing['has_missing'] = True
            missing['missing_fields'].append('race_categories')
        else:
            for idx, category in enumerate(race_categories):
                incomplete_fields = []

                if not category.get('name') or category.get('name') == 'null':
                    incomplete_fields.append('name')
                if not category.get('distance') or category.get('distance') == 'null':
                    incomplete_fields.append('distance')
                if not category.get('fee') or category.get('fee') == 'null':
                    incomplete_fields.append('fee')
                if not category.get('total_quota') or category.get('total_quota') == 'null':
                    incomplete_fields.append('total_quota')

                if incomplete_fields:
                    missing['has_missing'] = True
                    missing['incomplete_categories'].append({
                        'index': idx,
                        'category': category,
                        'missing_fields': incomplete_fields
                    })

        return missing

    def merge_completed_data(self, original_data: Dict, completed_data: Dict) -> Dict:
        """åˆå¹¶åŸå§‹æ•°æ®å’Œè¡¥å…¨æ•°æ®"""
        if not completed_data:
            return original_data

        merged = original_data.copy()

        # è¡¥å…¨æ€»è§„æ¨¡
        if completed_data.get('total_scale') and completed_data['total_scale'] != 'null':
            if not merged.get('total_scale') or merged.get('total_scale') == 'null':
                merged['total_scale'] = completed_data['total_scale']

        # è¡¥å…¨ç»„åˆ«
        if completed_data.get('race_categories'):
            if not merged.get('race_categories') or len(merged['race_categories']) == 0:
                merged['race_categories'] = completed_data['race_categories']
            else:
                # æ™ºèƒ½åˆå¹¶
                original_cats = merged['race_categories']
                completed_cats = completed_data['race_categories']

                for orig_cat in original_cats:
                    orig_name = self._normalize_category_name(orig_cat.get('name', ''))

                    for comp_cat in completed_cats:
                        comp_name = self._normalize_category_name(comp_cat.get('name', ''))

                        if orig_name and comp_name and self._is_similar_category_name(orig_name, comp_name):
                            # è¡¥å…¨ç¼ºå¤±å­—æ®µ
                            for field in ['distance', 'fee', 'total_quota']:
                                if (not orig_cat.get(field) or orig_cat.get(field) == 'null') and \
                                        comp_cat.get(field) and comp_cat.get(field) != 'null':
                                    orig_cat[field] = comp_cat[field]
                            break

                # æ·»åŠ æ–°ç»„åˆ«
                orig_names = {self._normalize_category_name(c.get('name', '')) for c in original_cats}
                for comp_cat in completed_cats:
                    comp_name = self._normalize_category_name(comp_cat.get('name', ''))
                    if comp_name and comp_name not in orig_names and self._is_valid_category(comp_cat):
                        original_cats.append(comp_cat)

                merged['race_categories'] = original_cats

        # è®°å½•è¡¥å…¨ä¿¡æ¯
        merged['completion_sources'] = completed_data.get('sources_used', [])
        merged['completion_confidence'] = completed_data.get('overall_confidence', 0.5)

        return merged

    def calculate_completion_rate(self, before: Dict, after: Dict) -> float:
        """è®¡ç®—è¡¥å…¨ç‡"""
        total_fields = 1  # æ€»è§„æ¨¡
        completed_fields = 0

        if not after.get('missing_total_scale'):
            completed_fields += 1

        # ç»Ÿè®¡ç»„åˆ«å­—æ®µ
        if before.get('incomplete_categories'):
            total_fields += len(before['incomplete_categories']) * 4  # æ¯ä¸ªç»„åˆ«4ä¸ªå­—æ®µ

            if after.get('incomplete_categories'):
                completed_fields += (len(before['incomplete_categories']) - len(after['incomplete_categories'])) * 4
            else:
                completed_fields += len(before['incomplete_categories']) * 4

        return (completed_fields / total_fields * 100) if total_fields > 0 else 100.0

    # ==================== æœç´¢å·¥å…· ====================

    def _baidu_search(self, query: str, num: int = 10) -> List[Dict]:
        """ç™¾åº¦æœç´¢"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            url = f"https://www.baidu.com/s?wd={quote(query)}&rn={num}"

            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            results = []

            for item in soup.find_all('div', class_='result')[:num]:
                try:
                    title_tag = item.find('h3') or item.find('a')
                    title = title_tag.get_text(strip=True) if title_tag else ''

                    abstract_tag = item.find('div', class_='c-abstract') or item.find('span',
                                                                                      class_='content-right_8Zs40')
                    abstract = abstract_tag.get_text(strip=True) if abstract_tag else ''

                    link_tag = item.find('a')
                    link = link_tag.get('href', '') if link_tag else ''

                    if title and abstract:
                        results.append({'title': title, 'abstract': abstract, 'link': link})
                except Exception as e:
                    continue

            return results

        except Exception as e:
            logger.error(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []

    def _sogou_wechat_search(self, query: str, num: int = 10) -> List[Dict]:
        """æœç‹—å¾®ä¿¡æœç´¢"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            url = f"https://weixin.sogou.com/weixin?type=2&query={quote(query)}"

            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            results = []

            for item in soup.find_all('div', class_='txt-box')[:num]:
                try:
                    title_tag = item.find('h3') or item.find('a')
                    title = title_tag.get_text(strip=True) if title_tag else ''

                    abstract_tag = item.find('p', class_='txt-info')
                    abstract = abstract_tag.get_text(strip=True) if abstract_tag else ''

                    if title and abstract:
                        results.append({'title': title, 'abstract': abstract, 'link': ''})
                except:
                    continue

            return results

        except Exception as e:
            logger.error(f"æœç‹—å¾®ä¿¡æœç´¢å¤±è´¥: {e}")
            return []

    def _extract_from_search_results(self, event_name: str, results: List[Dict],
                                     missing_info: Dict, source_hint: str = "") -> Optional[Dict]:
        """ä½¿ç”¨AIä»æœç´¢ç»“æœä¸­æå–ä¿¡æ¯"""
        if not results:
            return None

        try:
            from openai import OpenAI

            client = OpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                http_client=httpx.Client(
                    base_url=self.api_base,
                    follow_redirects=True,
                )
            )

            # æ•´ç†æœç´¢ç»“æœ
            search_content = ""
            for idx, result in enumerate(results[:10], 1):
                search_content += f"\nã€ç»“æœ{idx}ã€‘\n"
                search_content += f"æ ‡é¢˜: {result.get('title', '')}\n"
                search_content += f"æ‘˜è¦: {result.get('abstract', '')}\n"

            prompt = f"""è¯·ä»ä»¥ä¸‹æœç´¢ç»“æœä¸­æå–ã€Œ{event_name}ã€çš„å‡†ç¡®ä¿¡æ¯:

{search_content}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼:
{{
  "total_scale": "èµ›äº‹æ€»è§„æ¨¡(çº¯æ•°å­—)",
  "race_categories": [
    {{
      "name": "ç»„åˆ«åç§°",
      "distance": "è·ç¦»(å¦‚42.195km)",
      "fee": "è´¹ç”¨(çº¯æ•°å­—)",
      "total_quota": "åé¢(çº¯æ•°å­—)"
    }}
  ],
  "source": "ä¿¡æ¯æ¥æº({source_hint})",
  "confidence": "high/medium/low"
}}

è¦æ±‚:
1. åªæå–ç¡®å®šçš„ä¿¡æ¯,ä¸ç¡®å®šçš„å¡«null
2. æ•°å­—å¿…é¡»å‡†ç¡®
3. æ˜ç¡®æ ‡æ³¨å¯ä¿¡åº¦"""

            response = client.chat.completions.create(
                model=self.search_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ•°æ®æå–ä¸“å®¶,åªæå–ç¡®å®šçš„ä¿¡æ¯"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"ä»æœç´¢ç»“æœæå–ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def _build_precise_search_prompt(self, event_name: str, missing_info: Dict) -> str:
        """æ„å»ºç²¾ç¡®çš„æœç´¢æç¤ºè¯"""
        prompt_parts = [
            f"ã€ä»»åŠ¡ã€‘æœç´¢ã€Œ{event_name}ã€çš„å‡†ç¡®èµ›äº‹ä¿¡æ¯",
            "",
            "ã€æœç´¢ç­–ç•¥ã€‘",
            "1. ä¼˜å…ˆæœç´¢: å®˜æ–¹ç½‘ç«™ã€å®˜æ–¹å…¬ä¼—å·ã€å®˜æ–¹æŠ¥åå¹³å°",
            "2. å¤‡é€‰æ¥æº: é©¬æ‹‰æ¾ä¸“ä¸šåª’ä½“(é©¬æ‹‰é©¬æ‹‰ã€çˆ±ç‡ƒçƒ§ç­‰)",
            "3. å…³é”®è¯: ç«èµ›è§„ç¨‹ã€æŠ¥åç®€ç« ã€èµ›äº‹æ‰‹å†Œ",
            "",
            "ã€éœ€è¦è¡¥å…¨çš„ä¿¡æ¯ã€‘"
        ]

        if missing_info.get('missing_total_scale'):
            prompt_parts.append("âœ“ èµ›äº‹æ€»è§„æ¨¡(æ‰€æœ‰ç»„åˆ«æ€»äººæ•°)")

        if missing_info.get('incomplete_categories'):
            prompt_parts.append("âœ“ ç»„åˆ«è¯¦ç»†ä¿¡æ¯:")
            for item in missing_info['incomplete_categories']:
                cat = item['category']
                prompt_parts.append(f"  - {cat.get('name', 'æœªçŸ¥ç»„åˆ«')}: éœ€è¦{', '.join(item['missing_fields'])}")

        prompt_parts.extend([
            "",
            "ã€è¿”å›æ ¼å¼ã€‘",
            "{",
            '  "total_scale": "çº¯æ•°å­—",',
            '  "race_categories": [',
            '    {"name": "ç»„åˆ«å", "distance": "42.195km", "fee": "120.00", "total_quota": "5000"}',
            '  ],',
            '  "source": "ä¿¡æ¯æ¥æºURL",',
            '  "confidence": "high/medium/low"',
            "}",
            "",
            "ã€å…³é”®è¦æ±‚ã€‘",
            "1. å¿…é¡»æœç´¢çœŸå®ä¿¡æ¯,ä¸èƒ½ç¼–é€ ",
            "2. ä¼˜å…ˆå®˜æ–¹æ¸ é“",
            "3. æ‰¾ä¸åˆ°çš„å­—æ®µå¡«null",
            "4. æ ‡æ³¨ä¿¡æ¯æ¥æºå’Œå¯ä¿¡åº¦"
        ])

        return '\n'.join(prompt_parts)

    def _validate_ai_response(self, data: Dict, event_name: str) -> bool:
        """éªŒè¯AIè¿”å›çš„æ•°æ®åˆç†æ€§"""
        if not data:
            return False

        # æ£€æŸ¥æ•°æ®ç»“æ„
        if 'race_categories' in data:
            for cat in data['race_categories']:
                # éªŒè¯è·ç¦»æ ¼å¼
                if cat.get('distance') and cat['distance'] != 'null':
                    if not re.match(r'\d+\.?\d*km', str(cat['distance'])):
                        return False

                # éªŒè¯è´¹ç”¨èŒƒå›´
                if cat.get('fee') and cat['fee'] != 'null':
                    try:
                        fee = float(re.sub(r'[^\d.]', '', str(cat['fee'])))
                        if fee < 0 or fee > 1000:
                            return False
                    except:
                        return False

        return True

    def _is_valid_category(self, category: Dict) -> bool:
        """æ£€æŸ¥ç»„åˆ«æ˜¯å¦æœ‰æ•ˆ"""
        return (
                category.get('name') and category['name'] != 'null' and
                category.get('distance') and category['distance'] != 'null'
        )

    def _normalize_category_name(self, name: str) -> str:
        """è§„èŒƒåŒ–ç»„åˆ«åç§°"""
        if not name:
            return ''

        name = name.strip().lower()

        # ç»Ÿä¸€å…¨é©¬è¡¨è¿°
        if any(k in name for k in ['å…¨ç¨‹', 'å…¨é©¬', 'full', 'marathon', '42']):
            return 'å…¨ç¨‹é©¬æ‹‰æ¾'
        # ç»Ÿä¸€åŠé©¬è¡¨è¿°
        elif any(k in name for k in ['åŠç¨‹', 'åŠé©¬', 'half', '21']):
            return 'åŠç¨‹é©¬æ‹‰æ¾'
        # ç»Ÿä¸€10å…¬é‡Œ
        elif '10' in name or 'åå…¬é‡Œ' in name:
            return '10å…¬é‡Œ'
        # ç»Ÿä¸€5å…¬é‡Œ
        elif '5' in name or 'äº”å…¬é‡Œ' in name:
            return '5å…¬é‡Œ'

        return name

    def _is_similar_category_name(self, name1: str, name2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªç»„åˆ«åç§°æ˜¯å¦ç›¸ä¼¼"""
        n1 = self._normalize_category_name(name1)
        n2 = self._normalize_category_name(name2)
        return n1 == n2

    def _weighted_consensus(self, values: List[Dict]) -> str:
        """åŠ æƒå…±è¯†ç®—æ³•"""
        if not values:
            return 'null'

        if len(values) == 1:
            return str(values[0]['value'])

        # è®¡ç®—åŠ æƒå¹³å‡
        total_weight = sum(v['confidence'] for v in values)
        weighted_sum = sum(v['value'] * v['confidence'] for v in values)

        result = int(weighted_sum / total_weight) if total_weight > 0 else values[0]['value']
        return str(result)

    def _calculate_field_confidence(self, values: List[Dict]) -> float:
        """è®¡ç®—å­—æ®µç½®ä¿¡åº¦"""
        if not values:
            return 0.0

        if len(values) == 1:
            return values[0]['confidence']

        # å¤šæºéªŒè¯,æ£€æŸ¥ä¸€è‡´æ€§
        nums = [v['value'] for v in values]
        avg = sum(nums) / len(nums)

        # è®¡ç®—å˜å¼‚ç³»æ•°
        variations = [abs(n - avg) / avg for n in nums if avg > 0]
        avg_variation = sum(variations) / len(variations) if variations else 0

        # ç½®ä¿¡åº¦ = åŸºç¡€ç½®ä¿¡åº¦ * (1 - å˜å¼‚ç³»æ•°)
        base_confidence = sum(v['confidence'] for v in values) / len(values)
        consistency_factor = max(0, 1 - avg_variation)

        return base_confidence * consistency_factor